{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SimpleBoostingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_models, rounds=5, learning_rate=0.1, validation_fraction=0.1, random_state=42):\n",
    "        self.base_models = base_models\n",
    "        self.rounds = rounds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Asegurarse de que y sea un array 1D\n",
    "        y = np.ravel(y)\n",
    "\n",
    "        # Dividir los datos en conjuntos de entrenamiento y validación\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=self.random_state)\n",
    "\n",
    "        self.classes_ = np.unique(y_train)\n",
    "\n",
    "        # Inicializar pesos\n",
    "        weights = np.ones(len(y_train)) / len(y_train)\n",
    "\n",
    "        for i in range(self.rounds):\n",
    "            errors = np.zeros(len(self.base_models))\n",
    "            predictions = np.zeros((len(self.base_models), len(y_train)))\n",
    "\n",
    "            # Entrenar modelos base y calcular errores\n",
    "            for i, model in enumerate(self.base_models):\n",
    "                model.fit(X_train, y_train, sample_weight=weights)\n",
    "                pred = model.predict(X_train)\n",
    "                predictions[i] = (pred.ravel() > 0.5).astype(int)  # Convertir probabilidades a etiquetas\n",
    "                errors[i] = np.sum(weights * (predictions[i] != y_train.astype(int)))\n",
    "\n",
    "            # Seleccionar el mejor modelo\n",
    "            best_model_index = np.argmin(errors)\n",
    "            best_model = self.base_models[best_model_index]\n",
    "            best_pred = predictions[best_model_index]\n",
    "\n",
    "            # Calcular alpha\n",
    "            error = errors[best_model_index]\n",
    "            alpha = self.learning_rate * (np.log((1 - error) / error) + np.log(len(self.classes_) - 1))\n",
    "\n",
    "            # Actualizar pesos\n",
    "            weights *= np.exp(alpha * (best_pred != y_train.astype(int)))\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "            # Guardar modelo y alpha\n",
    "            self.models.append(best_model)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            # Evaluar en el conjunto de validación\n",
    "            val_pred = self.predict(X_val)\n",
    "            val_error = np.mean(val_pred != y_val.astype(int))\n",
    "            print(f\"Validation error after round {i} model {len(self.models)}: {val_error:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((len(self.models), X.shape[0]))\n",
    "        for i, model in enumerate(self.models):\n",
    "            pred = model.predict(X)\n",
    "            predictions[i] = (pred.ravel() > 0.5).astype(int)  # Convertir probabilidades a etiquetas\n",
    "\n",
    "        weighted_preds = np.sum(np.array(self.alphas)[:, np.newaxis] * predictions, axis=0)\n",
    "\n",
    "        return self.classes_[(weighted_preds > 0).astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "class CNNImageForgeryPredictorModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, compression_quality: int = 90):\n",
    "        self.compression_quality = compression_quality\n",
    "\n",
    "        model = Sequential([\n",
    "            Input(shape=(128, 128, 1)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "    def recomprimir_imagen_tf(self, imagen):\n",
    "        imagen_jpeg = tf.image.encode_jpeg(imagen, quality=self.compression_quality)\n",
    "        imagen_recomprimida = tf.image.decode_jpeg(imagen_jpeg)\n",
    "        return imagen_recomprimida\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        imagen_original = tf.image.decode_jpeg(image, channels=3)\n",
    "        image_compressed = self.recomprimir_imagen_tf(imagen_original)\n",
    "        diff = tf.abs(tf.cast(imagen_original, tf.int32) - tf.cast(image_compressed, tf.int32))\n",
    "        diferencia_gris = tf.image.rgb_to_grayscale(tf.cast(diff, tf.uint8))\n",
    "        resized = tf.image.resize(diferencia_gris, (128, 128))\n",
    "        return resized\n",
    "    \n",
    "    def prepare_dataset(self, X):\n",
    "        X_processed = [self.preprocess_image(image) for image in X]\n",
    "        return np.array(X_processed)\n",
    "\n",
    "    def fit(self, X, y, sample_weight):\n",
    "        X_processed = self.prepare_dataset(X)\n",
    "        self.model.fit(X_processed, y, sample_weight=sample_weight)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_processed = self.prepare_dataset(X)\n",
    "        return self.model.predict(X_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "class SVMImageForgeryPredictorModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.model = svm.SVC()\n",
    "\n",
    "    @staticmethod\n",
    "    def fourier_transform(image):\n",
    "        f = np.fft.fft2(image)\n",
    "        fshift = np.fft.fftshift(f)\n",
    "        magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "        magnitude_spectrum[np.isinf(magnitude_spectrum)] = 0  # Reemplazar infinitos con 0\n",
    "        return magnitude_spectrum\n",
    "    \n",
    "    @staticmethod\n",
    "    def noise_features(image):\n",
    "        # modelo de ruido básico\n",
    "        mean_noise = np.mean(image)\n",
    "        std_noise = np.std(image)\n",
    "        return mean_noise, std_noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def edge_detection(image):\n",
    "        edges = cv2.Canny(image, 100, 200)\n",
    "        return edges\n",
    "    \n",
    "    @staticmethod\n",
    "    def texture_features(image):\n",
    "        g = graycomatrix(image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256)\n",
    "        contrast = graycoprops(g, 'contrast')\n",
    "        return np.mean(contrast)\n",
    "    \n",
    "    @staticmethod\n",
    "    # Compatible with grey scale \n",
    "    def segment_image(image, k=4):\n",
    "        # Flatten the image to a 1D array suitable for k-means\n",
    "        Z = image.reshape((-1, 1))\n",
    "\n",
    "        # Convert to float32\n",
    "        Z = np.float32(Z)\n",
    "\n",
    "        # Criteria and k-means application\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "        ret, label, center = cv2.kmeans(Z, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "        # Convert back to uint8 and map centers to the original image\n",
    "        center = np.uint8(center)\n",
    "        res = center[label.flatten()]\n",
    "        segmented_image = res.reshape((image.shape))\n",
    "\n",
    "        return segmented_image\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        ft = self.fourier_transform(image).ravel()  # Aplana el resultado de la transformada de Fourier 70\n",
    "        nf = [], []#noise_features(image)  # Retorna dos escalares 72\n",
    "        ed = []#edge_detection(image).ravel()  # Aplana los bordes detectados 77\n",
    "        tf = []#np.array([texture_features(image)])  # Envuelve el escalar en un arreglo 72\n",
    "        seg = self.segment_image(image).ravel()  # Aplana la imagen segmentada 58\n",
    "        \n",
    "        # Concatena todas las características en un solo arreglo 1D\n",
    "        return np.hstack([ft, nf[0], nf[1], ed, tf, seg])\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.rgb_to_grayscale(tf.cast(image, tf.float32))\n",
    "        image = tf.image.resize(image, (256, 384))\n",
    "        image = image.numpy()\n",
    "        feat = self.extract_features(image)\n",
    "        return feat\n",
    "\n",
    "    def prepare_dataset(self, images):\n",
    "        images = np.array(images)\n",
    "        features = [self.preprocess_image(image) for image in images]\n",
    "        return np.array(features)\n",
    "    \n",
    "    def fit(self, X, y, sample_weight):\n",
    "        X_processed = self.prepare_dataset(X)\n",
    "        self.model.fit(X_processed, y, sample_weight=sample_weight)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_processed = self.prepare_dataset(X)\n",
    "        return self.model.predict(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class TransferLearningImageForgeryPredictorModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, img_size=(160, 160), dropout_rate=0.2, learning_rate=0.0001):\n",
    "        ## Configuración de la imagen\n",
    "        self.img_size = img_size\n",
    "        self.img_shape = self.img_size + (3,)\n",
    "\n",
    "        ## Data Augmentation\n",
    "        self.data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.RandomFlip('horizontal'),\n",
    "            tf.keras.layers.RandomRotation(0.2),\n",
    "        ])\n",
    "        self.preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "        ## Modelo base\n",
    "        self.base_model = tf.keras.applications.MobileNetV2(input_shape=self.img_shape,\n",
    "                                                           include_top=False,\n",
    "                                                           weights='imagenet')\n",
    "        self.base_model.trainable = False\n",
    "\n",
    "        ## Capas adicionales\n",
    "        self.global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "        ## Construcción del modelo\n",
    "        self.inputs = tf.keras.Input(shape=self.img_shape)\n",
    "        x = self.data_augmentation(self.inputs)\n",
    "        x = self.preprocess_input(x)\n",
    "        x = self.base_model(x, training=False)\n",
    "        x = self.global_average_layer(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        self.outputs = self.prediction_layer(x)\n",
    "        self.model = tf.keras.Model(self.inputs, self.outputs)\n",
    "\n",
    "        ## Compilación del modelo\n",
    "        self.base_learning_rate = learning_rate\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.base_learning_rate),\n",
    "                          loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                          metrics=['accuracy'])\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, self.img_size)\n",
    "        return image\n",
    "    \n",
    "    def prepare_dataset(self, X):\n",
    "        X_processed = [self.preprocess_image(image) for image in X]\n",
    "        return np.array(X_processed)\n",
    "\n",
    "    def fit(self, X, y, sample_weight):\n",
    "        X_processed = self.prepare_dataset(X)\n",
    "\n",
    "        initial_epochs = 10\n",
    "        history = self.model.fit(X_processed, y, epochs=initial_epochs, sample_weight=sample_weight)\n",
    "        self.base_model.trainable = True\n",
    "\n",
    "        # Fine-tune from this layer onwards\n",
    "        fine_tune_at = 100\n",
    "\n",
    "        # Freeze all the layers before the `fine_tune_at` layer\n",
    "        for layer in self.base_model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        self.model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.base_learning_rate/10),\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        fine_tune_epochs = 10\n",
    "        total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "        self.model.fit(X_processed, y, epochs=total_epochs, initial_epoch=history.epoch[-1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_processed = self.prepare_dataset(X)\n",
    "        return self.model.predict(X_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Path to your CASIA2 dataset\n",
    "PATH = '../data/CASIA2'\n",
    "\n",
    "# Directories for authentic and tampered images\n",
    "authentic_dir = os.path.join(PATH, 'Au')\n",
    "tampered_dir = os.path.join(PATH, 'Tp2')\n",
    "\n",
    "authentic_number = 100\n",
    "tampered_number = 100\n",
    "\n",
    "def load_images_from_directory(directory_path, n, i=0):\n",
    "    images = []\n",
    "    for filename in os.listdir(directory_path)[i:n]:\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(directory_path, filename)\n",
    "            imagen_original = tf.io.read_file(image_path)\n",
    "            images.append(imagen_original)\n",
    "    return images\n",
    "\n",
    "# Get file lists and labels\n",
    "authentic_files = load_images_from_directory(authentic_dir, authentic_number)\n",
    "tampered_files = load_images_from_directory(tampered_dir, tampered_number)\n",
    "authentic_labels = [0] * len(authentic_files)\n",
    "tampered_labels = [1] * len(tampered_files)\n",
    "\n",
    "# Combine authentic and tampered data\n",
    "all_files = authentic_files + tampered_files\n",
    "all_labels = authentic_labels + tampered_labels\n",
    "\n",
    "all_files = np.array(all_files)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_files, all_labels, test_size=0.4, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Crear modelos base\n",
    "base_models = [TransferLearningImageForgeryPredictorModel() for _ in range(2)] + [SVMImageForgeryPredictorModel() for _ in range(2)] + [CNNImageForgeryPredictorModel() for _ in range(2)]\n",
    "\n",
    "# Crear y entrenar el modelo de boosting con early stopping\n",
    "boosting_model = SimpleBoostingClassifier(\n",
    "    base_models, \n",
    "    rounds=1\n",
    ")\n",
    "boosting_model.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar\n",
    "y_pred = boosting_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anabelbg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
