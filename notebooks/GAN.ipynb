{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, LeakyReLU, MaxPooling2D, Cropping2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose, BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9614 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Configura el ImageDataGenerator para el entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normaliza las imágenes\n",
    "    rotation_range=20,  # Añade algo de rotación para robustez\n",
    "    width_shift_range=0.2,  # Desplazamientos horizontales\n",
    "    height_shift_range=0.2,  # Desplazamientos verticales\n",
    "    shear_range=0.2,  # Cizallamiento\n",
    "    zoom_range=0.2,  # Zoom\n",
    "    horizontal_flip=True,  # Invierte las imágenes horizontalmente\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Suponiendo que tus carpetas se llaman 'not_manipulated' y 'manipulated'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:\\dataset--ml\\CASIA2-Train',  # Asegúrate de que esta ruta es accesible y correcta\n",
    "    target_size=(150, 150),  # Redimensiona las imágenes a 150x150\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # 'binary' porque tienes dos clases, manipulado y no manipulado\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator(input_shape):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    \n",
    "    base_model.trainable = True\n",
    "\n",
    "    fine_tune_at = 100\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip('horizontal'),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "    ])\n",
    "\n",
    "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = data_augmentation(inputs)\n",
    "    x = preprocess_input(x)\n",
    "    x = base_model(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    #x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    # model = Sequential([\n",
    "    #     Conv2D(64, (5, 5), padding='same', input_shape=input_shape),\n",
    "    #     LeakyReLU(alpha=0.2),\n",
    "    #     MaxPooling2D(pool_size=(2, 2)),\n",
    "    #     Conv2D(128, (5, 5)),\n",
    "    #     LeakyReLU(alpha=0.2),\n",
    "    #     MaxPooling2D(pool_size=(2, 2)),\n",
    "    #     Flatten(),\n",
    "    #     Dense(50, activation='sigmoid'),\n",
    "    #     Dense(1, activation='sigmoid')\n",
    "    # ])\n",
    "\n",
    "    # def custom_loss(y_true, y_pred):\n",
    "    #     # Penaliza las salidas cercanas a 0.5\n",
    "    #     penalty = tf.abs(y_pred - 0.5)\n",
    "    #     return tf.keras.losses.binary_crossentropy(y_true, y_pred) + penalty\n",
    "\n",
    "    #model.compile(loss=custom_loss, optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00001),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator((150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el discriminador\n",
    "history = discriminator.fit(\n",
    "    train_generator,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura el ImageDataGenerator para el entrenamiento\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    'D:\\dataset--ml\\CASIA2-Test',  # Asegúrate de que esta ruta es accesible y correcta\n",
    "    target_size=(150, 150),  # Redimensiona las imágenes a 150x150\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # 'binary' porque tienes dos clases, manipulado y no manipulado\n",
    ")\n",
    "\n",
    "loss, accuracy = discriminator.evaluate(test_generator, steps=len(test_generator))\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim):\n",
    "    model = Sequential([\n",
    "        # Asegúrate de que la salida de la capa densa se redimensiona correctamente\n",
    "        Dense(128 * 38 * 38, activation=\"relu\", input_dim=latent_dim),  # Ajuste basado en el cálculo a continuación\n",
    "        Reshape((38, 38, 128)),  # Comienza desde una base que pueda escalarse correctamente\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # Primera capa transpuesta para escalar de 38x38 a 76x76\n",
    "        Conv2DTranspose(128, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # Segunda capa transpuesta, escala de 76x76 a 152x152, necesitaremos recortar después\n",
    "        Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # Capa final para ajustar a 150x150 y reducir a 3 canales para la imagen RGB\n",
    "        Conv2DTranspose(3, kernel_size=3, strides=1, padding='valid', activation='sigmoid'),  # Usando 'valid' para ajustar exactamente\n",
    "        # Recorte las dimensiones para que coincidan con 150x150x3\n",
    "        Cropping2D(cropping=((2, 2), (2, 2)))  # Recorta 1 pixel de cada borde\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    # Asegurarse de que el discriminador no sea entrenable cuando se entrena la GAN\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # Entrada de ruido al generador\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    # Imagen generada\n",
    "    img = generator(z)\n",
    "    # Discriminador decide si la imagen es real o falsa\n",
    "    valid = discriminator(img)\n",
    "\n",
    "    # Modelo completo\n",
    "    model = Model(inputs=z, outputs=valid)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crea la GAN combinando generador y discriminador\n",
    "gan = build_gan(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9614 images belonging to 2 classes.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "Epoch 1/65 - D loss: [2.26146752 0.34375   ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 631ms/step\n",
      "Epoch 2/65 - D loss: [0.90408236 0.3125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 640ms/step\n",
      "Epoch 3/65 - D loss: [0.55435243 0.828125  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 645ms/step\n",
      "Epoch 4/65 - D loss: [0.44555144 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 617ms/step\n",
      "Epoch 5/65 - D loss: [0.45562802 0.8125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 647ms/step\n",
      "Epoch 6/65 - D loss: [0.40338811 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 621ms/step\n",
      "Epoch 7/65 - D loss: [0.38183238 0.828125  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 717ms/step\n",
      "Epoch 8/65 - D loss: [0.44696935 0.765625  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 724ms/step\n",
      "Epoch 9/65 - D loss: [0.43369433 0.796875  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 738ms/step\n",
      "Epoch 10/65 - D loss: [0.3515253 0.859375 ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 790ms/step\n",
      "Epoch 11/65 - D loss: [0.36730504 0.828125  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 649ms/step\n",
      "Epoch 12/65 - D loss: [0.43826097 0.796875  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "Epoch 13/65 - D loss: [0.41354134 0.78125   ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "Epoch 14/65 - D loss: [0.34057297 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 598ms/step\n",
      "Epoch 15/65 - D loss: [0.36607525 0.84375   ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 683ms/step\n",
      "Epoch 16/65 - D loss: [0.50433835 0.71875   ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 600ms/step\n",
      "Epoch 17/65 - D loss: [0.41424628 0.796875  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 678ms/step\n",
      "Epoch 18/65 - D loss: [0.32186519 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 596ms/step\n",
      "Epoch 19/65 - D loss: [0.47740321 0.734375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 608ms/step\n",
      "Epoch 20/65 - D loss: [0.40359721 0.8125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 668ms/step\n",
      "Epoch 21/65 - D loss: [0.40146979 0.8125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 726ms/step\n",
      "Epoch 22/65 - D loss: [0.37642646 0.796875  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 640ms/step\n",
      "Epoch 23/65 - D loss: [0.3470263 0.84375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 682ms/step\n",
      "Epoch 24/65 - D loss: [0.32155366 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 780ms/step\n",
      "Epoch 25/65 - D loss: [0.32764999 0.875     ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 545ms/step\n",
      "Epoch 26/65 - D loss: [0.40135853 0.765625  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 622ms/step\n",
      "Epoch 27/65 - D loss: [0.32483065 0.84375   ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 540ms/step\n",
      "Epoch 28/65 - D loss: [0.36030636 0.84375   ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 580ms/step\n",
      "Epoch 29/65 - D loss: [0.42124482 0.765625  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 538ms/step\n",
      "Epoch 30/65 - D loss: [0.38317533 0.765625  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 675ms/step\n",
      "Epoch 31/65 - D loss: [0.32528652 0.8125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 604ms/step\n",
      "Epoch 32/65 - D loss: [0.31075801 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "Epoch 33/65 - D loss: [0.38719666 0.78125   ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 590ms/step\n",
      "Epoch 34/65 - D loss: [0.32433144 0.828125  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 654ms/step\n",
      "Epoch 35/65 - D loss: [0.39082417 0.765625  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 611ms/step\n",
      "Epoch 36/65 - D loss: [0.41651496 0.75      ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 587ms/step\n",
      "Epoch 37/65 - D loss: [0.39139006 0.75      ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "Epoch 38/65 - D loss: [0.36946604 0.8125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 579ms/step\n",
      "Epoch 39/65 - D loss: [0.37698282 0.796875  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "Epoch 40/65 - D loss: [0.37990542 0.734375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 674ms/step\n",
      "Epoch 41/65 - D loss: [0.3261766 0.875    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 611ms/step\n",
      "Epoch 42/65 - D loss: [0.36723024 0.78125   ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 625ms/step\n",
      "Epoch 43/65 - D loss: [0.35624682 0.8125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 686ms/step\n",
      "Epoch 44/65 - D loss: [0.31136738 0.890625  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 804ms/step\n",
      "Epoch 45/65 - D loss: [0.33875968 0.828125  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 659ms/step\n",
      "Epoch 46/65 - D loss: [0.401186 0.75    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 615ms/step\n",
      "Epoch 47/65 - D loss: [0.32407257 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 657ms/step\n",
      "Epoch 48/65 - D loss: [0.38173769 0.796875  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 690ms/step\n",
      "Epoch 49/65 - D loss: [0.40669828 0.75      ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 665ms/step\n",
      "Epoch 50/65 - D loss: [0.32332599 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 707ms/step\n",
      "Epoch 51/65 - D loss: [0.38384342 0.765625  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 609ms/step\n",
      "Epoch 52/65 - D loss: [0.33733364 0.796875  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 614ms/step\n",
      "Epoch 53/65 - D loss: [0.37241677 0.75      ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 638ms/step\n",
      "Epoch 54/65 - D loss: [0.35212473 0.8125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 754ms/step\n",
      "Epoch 55/65 - D loss: [0.33792863 0.796875  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 656ms/step\n",
      "Epoch 56/65 - D loss: [0.34528367 0.78125   ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 649ms/step\n",
      "Epoch 57/65 - D loss: [0.36526303 0.8125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 801ms/step\n",
      "Epoch 58/65 - D loss: [0.34084003 0.8125    ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 726ms/step\n",
      "Epoch 59/65 - D loss: [0.30006595 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 673ms/step\n",
      "Epoch 60/65 - D loss: [0.3626952 0.765625 ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 600ms/step\n",
      "Epoch 61/65 - D loss: [0.36752788 0.796875  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 638ms/step\n",
      "Epoch 62/65 - D loss: [0.34357679 0.828125  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 801ms/step\n",
      "Epoch 63/65 - D loss: [0.32767865 0.828125  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 746ms/step\n",
      "Epoch 64/65 - D loss: [0.31551262 0.859375  ] - G loss: 15.424948692321777\n",
      "1/1 [==============================] - 1s 721ms/step\n",
      "Epoch 65/65 - D loss: [0.37205126 0.796875  ] - G loss: 15.424948692321777\n"
     ]
    }
   ],
   "source": [
    "def train_gan(generator, discriminator, gan, epochs, batch_size, save_interval):\n",
    "    for epoch in range(epochs):\n",
    "        # Obtener un lote de imágenes no manipuladas\n",
    "        real_images, labels = next(train_gan_generator)\n",
    "        length = len(real_images)\n",
    "\n",
    "        # Generar ruido aleatorio\n",
    "        noise = np.random.normal(0, 1, (length, latent_dim))\n",
    "\n",
    "        # Generar un lote de nuevas imágenes\n",
    "        generated_images = generator.predict(noise)\n",
    "\n",
    "        # Preparar etiquetas para el lote real y falso\n",
    "        valid = np.ones((length, 1))\n",
    "        fake = np.zeros((length, 1))\n",
    "\n",
    "        # Entrenar el discriminador (real clasificado como 1 y falso como 0)\n",
    "        d_loss_real = discriminator.train_on_batch(real_images, labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_images, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Entrenar el generador (tratamos de engañar al discriminador para clasificar falsos como reales)\n",
    "        g_loss = gan.train_on_batch(noise, valid)\n",
    "\n",
    "        # Progreso de impresión y guardado ocasional de imágenes\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - D loss: {d_loss} - G loss: {g_loss}\")\n",
    "        \n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            #save_images(epoch, generator)\n",
    "            pass\n",
    "\n",
    "def save_images(epoch, generator, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, (dim[0] * dim[1], latent_dim))\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(dim[0] * dim[1], 150, 150, 3)\n",
    "    \n",
    "    # Configurar la cuadrícula de imágenes\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(dim[0] * dim[1]):\n",
    "        plt.subplot(dim[0], dim[1], i + 1)\n",
    "        plt.imshow(generated_images[i])\n",
    "        plt.axis('off')  # No mostrar ejes\n",
    "    \n",
    "    plt.tight_layout()  # Ajustar el layout para minimizar el espacio entre imágenes\n",
    "    plt.show()  # Mostrar todas las imágenes generadas\n",
    "\n",
    "\n",
    "# Configurar parámetros\n",
    "epochs = 65\n",
    "batch_size = 32\n",
    "save_interval = 10\n",
    "\n",
    "train_gan_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_gan_generator = train_datagen.flow_from_directory(\n",
    "    'D:\\dataset--ml\\CASIA2-Train',  # Asegúrate de que apunta a la carpeta correcta\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  # Sin etiquetas ya que sólo cargamos imágenes\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Llamar a train_gan\n",
    "train_gan(generator, discriminator, gan, epochs, batch_size, save_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 2 classes.\n",
      "94/94 [==============================] - 48s 508ms/step - loss: 0.7537 - accuracy: 0.5000\n",
      "Test Loss: 0.7536779046058655\n",
      "Test Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Configura el ImageDataGenerator para el entrenamiento\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    'D:\\dataset--ml\\CASIA2-Test',  # Asegúrate de que esta ruta es accesible y correcta\n",
    "    target_size=(150, 150),  # Redimensiona las imágenes a 150x150\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # 'binary' porque tienes dos clases, manipulado y no manipulado\n",
    ")\n",
    "\n",
    "loss, accuracy = discriminator.evaluate(test_generator, steps=len(test_generator))\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Probabilidad de que la imagen sea falsa: -0.64\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "\n",
    "# Cargar y preparar la imagen\n",
    "def prepare_image(image_path, target_size=(150, 150)):\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array / 255.0  # Normalizar a 0-1\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Añadir batch dimension\n",
    "    return img_array\n",
    "\n",
    "image_path = 'D:\\dataset--ml\\CASIA2-Train\\Au\\Au_ani_00036.jpg'\n",
    "prepared_image = prepare_image(image_path)\n",
    "\n",
    "def predict_image_realness(model, image):\n",
    "    prediction = model.predict(image)\n",
    "    return prediction[0][0]  # Devuelve la probabilidad de que la imagen sea real\n",
    "\n",
    "# Suponiendo que `discriminator` es tu modelo entrenado\n",
    "realness_prob = predict_image_realness(discriminator, prepared_image)\n",
    "print(f\"Probabilidad de que la imagen sea falsa: {realness_prob:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
